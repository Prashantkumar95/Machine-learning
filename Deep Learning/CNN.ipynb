{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81d17877-83cb-4da3-b6cd-521bb09eac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c2f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a831dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbb1003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(Conv2D(32,(3,3),input_shape=(64,64,3),activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=(2,2)))\n",
    "cnn.add(Conv2D(16,(3,3),activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=(2,2)))\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e73a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(64,activation='relu'))\n",
    "cnn.add(Dense(32,activation='relu'))\n",
    "cnn.add(Dense(16,activation='relu'))\n",
    "cnn.add(Dense(8,activation='relu'))\n",
    "cnn.add(Dense(4,activation='relu'))\n",
    "cnn.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccde93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='binary_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c7cb0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4s/step - loss: 0.6995 - val_loss: 0.7007\n",
      "Epoch 2/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - loss: 0.7020 - val_loss: 0.6971\n",
      "Epoch 3/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - loss: 0.6969 - val_loss: 0.6962\n",
      "Epoch 4/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 911ms/step - loss: 0.6918 - val_loss: 0.6948\n",
      "Epoch 5/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 776ms/step - loss: 0.6990 - val_loss: 0.6934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22692ba2fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        r\"C:\\Users\\HP\\Downloads\\CNN_Dataset\\training_set\",\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        r\"C:\\Users\\HP\\Downloads\\CNN_Dataset\\test_set\",\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=200 // 32,     # 32 = batch size\n",
    "    epochs=5,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=800 // 32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ac2bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f380787",
   "metadata": {},
   "outputs": [],
   "source": [
    "img =image.load_img(r\"C:\\Users\\HP\\Downloads\\CNN_Dataset\\single_prediction\\cat_or_dog_1.jpg\",target_size=(64,64))\n",
    "img1 =image.load_img(r\"C:\\Users\\HP\\Downloads\\CNN_Dataset\\single_prediction\\cat_or_dog_2.jpg\",target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5144e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.img_to_array(img)\n",
    "img1=image.img_to_array(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218d90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 54.,  58.,   7.],\n",
       "        [ 58.,  63.,   9.],\n",
       "        [ 64.,  67.,  10.],\n",
       "        ...,\n",
       "        [136., 144.,  71.],\n",
       "        [140., 150.,  77.],\n",
       "        [139., 149.,  78.]],\n",
       "\n",
       "       [[ 48.,  54.,   6.],\n",
       "        [ 51.,  58.,   7.],\n",
       "        [ 58.,  63.,   9.],\n",
       "        ...,\n",
       "        [129., 137.,  64.],\n",
       "        [139., 149.,  78.],\n",
       "        [141., 151.,  80.]],\n",
       "\n",
       "       [[ 48.,  56.,   7.],\n",
       "        [ 48.,  56.,   7.],\n",
       "        [ 54.,  61.,  10.],\n",
       "        ...,\n",
       "        [123., 129.,  65.],\n",
       "        [136., 145.,  80.],\n",
       "        [140., 149.,  82.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 46.,  55.,  12.],\n",
       "        [ 42.,  50.,  11.],\n",
       "        [ 38.,  49.,   9.],\n",
       "        ...,\n",
       "        [239., 205., 170.],\n",
       "        [235., 209., 186.],\n",
       "        [231., 201., 173.]],\n",
       "\n",
       "       [[ 50.,  57.,  13.],\n",
       "        [ 42.,  50.,  11.],\n",
       "        [ 44.,  52.,  11.],\n",
       "        ...,\n",
       "        [234., 200., 163.],\n",
       "        [236., 206., 178.],\n",
       "        [234., 203., 175.]],\n",
       "\n",
       "       [[ 53.,  59.,  13.],\n",
       "        [ 43.,  51.,  10.],\n",
       "        [ 49.,  56.,  12.],\n",
       "        ...,\n",
       "        [233., 195., 159.],\n",
       "        [235., 213., 190.],\n",
       "        [233., 206., 179.]]], shape=(64, 64, 3), dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7a32874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[239., 239., 239.],\n",
       "        [239., 239., 239.],\n",
       "        [239., 239., 239.],\n",
       "        ...,\n",
       "        [245., 245., 245.],\n",
       "        [245., 245., 245.],\n",
       "        [245., 245., 245.]],\n",
       "\n",
       "       [[239., 239., 239.],\n",
       "        [239., 239., 239.],\n",
       "        [239., 239., 239.],\n",
       "        ...,\n",
       "        [245., 245., 245.],\n",
       "        [245., 245., 245.],\n",
       "        [244., 244., 244.]],\n",
       "\n",
       "       [[239., 239., 239.],\n",
       "        [239., 239., 239.],\n",
       "        [239., 239., 239.],\n",
       "        ...,\n",
       "        [244., 244., 244.],\n",
       "        [244., 244., 244.],\n",
       "        [244., 244., 244.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[241., 217., 205.],\n",
       "        [243., 220., 206.],\n",
       "        [242., 219., 205.],\n",
       "        ...,\n",
       "        [226., 201., 181.],\n",
       "        [233., 205., 193.],\n",
       "        [232., 215., 197.]],\n",
       "\n",
       "       [[255., 236., 229.],\n",
       "        [251., 234., 227.],\n",
       "        [252., 235., 227.],\n",
       "        ...,\n",
       "        [219., 193., 178.],\n",
       "        [219., 193., 178.],\n",
       "        [216., 189., 172.]],\n",
       "\n",
       "       [[243., 230., 224.],\n",
       "        [250., 235., 228.],\n",
       "        [245., 232., 226.],\n",
       "        ...,\n",
       "        [203., 180., 164.],\n",
       "        [220., 198., 177.],\n",
       "        [225., 202., 186.]]], shape=(64, 64, 3), dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c277a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=np.expand_dims(img,axis=0)\n",
    "img1=np.expand_dims(img1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83a7c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step\n",
      "It's a cat ğŸ±\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img_path = r\"C:\\Users\\HP\\Downloads\\CNN_Dataset\\single_prediction\\cat_or_dog_1.jpg\"\n",
    "img = image.load_img(img_path, target_size=(64, 64))\n",
    "\n",
    "# Convert to numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Normalize (scale pixels 0â€“1)\n",
    "img_array = img_array / 255.0\n",
    "\n",
    "# Expand dimensions to add batch axis: (1, 64, 64, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Now predict\n",
    "prediction = cnn.predict(img_array)\n",
    "\n",
    "# Interpret result\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"It's a dog ğŸ¶\")\n",
    "else:\n",
    "    print(\"It's a cat ğŸ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1668d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "if p[0][0]<0.5:\n",
    "    print(\"Dog\")\n",
    "else:\n",
    "    print(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4cb6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
